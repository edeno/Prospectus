### Specific [Aim \#2](#aim2): To understand the functional contribution of ACC and dlPFC neurons to circuit-level dynamics and behavior during context switching {#aim2}
In [Aim \#1](#aim1), our goal is to understanding how groups of neurons interact at the circuit level. In [Aim \#2](#aim2), our goal is to characterize the cellular-level contributions of ACC and dlPFC neurons and their relationship to the circuit-level activity. This is important because it relates how activity in ACC and dlPFC works at different spatial scales.

#### Statistical methods: generalized linear models
Our approach is to employ multi-dimensional generalized linear regression models [@mccullagh_generalized_1989] that account for many task factors simultaneously. In most analyses, linear regressions with Gaussian noise or ANOVAs are used to partition the contributions of task factors on neuronal firing rate. While this is often times adequate if the signal-to-noise ratio is high, generalized linear models make more efficient use of the data in several ways.

First, firing rates are strictly positive. They are bounded below by zero. In a linear model or ANOVA, this can cause problems because these models can predict negative firing rates, thus overestimating the contribution of a task factor. This is particularly a problem for cortical neurons, because the distribution of firing rates over a population of neurons is typically skewed toward lower firing rates [@buzsaki_logdynamic_2014]. *Generalized* linear models allow for a non-linear transformation of the data so that all predictions are strictly positive.

Second, spike counts of neurons are discrete (spikes are typically recorded as spike or no spike over a specified time range). The most common distribution used to model variation in discrete counts is the Poisson distribution. Generalized linear models allow for the use of the Poisson distribution whereas the normal linear model and ANOVA do not.

Finally, generalized linear models are an extremely flexible framework. They can incorporate and efficiently estimate effects of spike history (refractory period, bursting), non-linear firing rate changes over time and multiple factors, and dependence on other neurons or groups of neurons (e.g. estimate the contribution of other neurons or groups of neurons to the firing rate of a single neuron). [@fig:figure3] shows the ability of generalized to track  non-linear changes in firing rate over multiple factors (simulated data). This is important for accurately describing neurons that may have multi-dimensional receptive fields.

![Demonstration of the flexibility of generalized linear models on simulated data. (A) Fit of the generalized linear model (blue line) on simulated data (red line) over a segment of trials. The generalized linear model is able to track the fluctuations in firing rate over time and trials. (B) The model tracks non-linear firing rate changes over a simulated Rule factor. (C) The model tracks non-linear firing rate changes over a simulated Response Direction factor.](figures/Figure3.png){#fig:figure3}

#### Why multi-dimensional models for prefrontal cortical neurons?
The most common neuron in prefrontal cortex, like other cortical areas, is the pyramidal neuron --- 70-85% of all neurons in cortex are pyramidal [@defelipe_pyramidal_1992]. Because of this, most neurons recorded by an electrode are pyramidal. Therefore, when considering how to statistically model prefrontal neurons, we should consider the properties of pyramidal neurons in prefrontal cortex.

Pyramidal neurons in prefrontal cortex have more spines than those in other cortical areas [@elston_cortex_2003], meaning individual neurons have a greater capacity to integrate input than other areas. Prefrontal neurons also receive convergent input from many other cortical areas [@schwartz_callosal_1984], meaning each neuron is potentially influenced by multiple types of information. This is borne out in recordings of prefrontal neurons, which exhibit heterogeneous selectivity – they are often sensitive to many parameters of a task [@rigotti_importance_2013].

While this multi-dimensional representation seems to be beneficial to encoding a large number of task rules [@rigotti_internal_2010], it also implies that care should be taken when a task involves many different factors. Failing to account for factors simultaneously may overestimate their contribution.

#### Fitting a collection of models instead of only one model
As we described previously, the role of ACC neurons in task switching and in general is highly debated and more than one factor may affect the firing rate of prefrontal neurons. This leads to the statistical problem of model selection --- determining the set of task factors which "best" describe the neuron's firing. Fitting too many irrelevant task factors will result in lack of generalizability of the results. Fitting too few may overestimate the contribution of task factors. Determining what is the "best" is a difficult problem that typically is solved by compromising between bias (imposing strong assumptions on which task factors are involved or by invoking Occam's razor style arguments) and variance (allowing for more model flexibility in terms of degrees of freedom and fitting more task factors). Selection of the best model --- based on, for example, how well the model predicts future datasets --- can often ignore equally plausible models for the data.

Our approach, following @wickham_visualizing_2015, is to fit a collection of models, ranging from a simple model --- constant firing rate over time and no task factors affecting the neurons to a complex model --- attentional task factors (as in [@tbl:table1]) that may or may not impact the firing rate depending on the context. This approach does not escape the bias-variance tradeoff in model selection, but it does allow for several candidate best models, which we feel is more appropriate when uncertainty about the theory underlying functioning in ACC is so high.

#### Evaluation of the models – cross-validation, area under the curve, and posterior predictive checks
As a precaution against overfitting noise, we will examine the performance of the model in predicting withheld data versus simpler models. This will measure the ability of our model to generalize to new data. We will do this using five-fold cross validation, which means we will examine how a model fit on four-fifths of the data can predict performance on a withheld fifth of the data --- alternating the particular withheld fifth of the data and averaging the predictive performance over all of them.

To evaluate predictive performance, we will use a receiver operating characteristic measure called Area Under the Curve (AUC). In this measure, the predicted firing rate distribution when a spike occurs is compared to the predicted firing rate distribution when no spike occurs. AUC measures how discriminable the two populations of firing rates (spike versus no spike) are from each other. One advantage of using AUC compared to other methods is that AUCs are easily comparable from neuron to neuron [@kelly_accounting_2010], making it easier to evaluate how a model is doing over a population of neurons.

Finally, we will visually compare the model-generated data (spikes simulated from the model) to the real data. This technique is known as posterior predictive checking [@rubin_bayesianly_1984; @gelman_posterior_1996]. Posterior predictive checks are well-suited for understanding how complex models relate to the dataset, because they are simple to compute and diagnosing model misfit is a simple visual comparison. An example of this is in @pillow_spatiotemporal_2008 (Figure 3) where they compare models of retinal ganglion cells that take into account coupling from nearby cells to models that do not take into account coupling from nearby cells.

#### Comparisons and Outcomes
To examine context selectivity of individual neurons in relation to attention factors, we will fit a collection of models ranging from assuming no change in firing rate to including all factors described in [@tbl:table1] and their interaction with Rule. For example, with just two factors (for example, Rule and Previous Error History), the collection of models would be: the constant model (no change in firing rate during the task), Rule, Previous Error History, Rule and Previous Error History, and Rule interacted with Previous Error History.  If successful, this will give us a thorough characterization of how each attentionally demanding factor changes the firing rate and whether this change in firing rate is dependent on rule.

To examine how the population firing rates of ACC and dlPFC influence individual neurons, we will fit models that include the population spiking history of ACC neurons and dlPFC neurons where the population spiking history consists of the summed spikes of all neurons in a given recording session for a given area. If successful, this will determine if the influence of population firing from ACC and dlPFC changes with rule and attentionally demanding task factors.

#### Potential Pitfalls and Alternative Approaches
The models we propose to examine are complex and our ability to estimate them successfully relies on the power of the data. Additionally, complex models can be harder to interpret. We have proposed two checks on the complexity of our model: the performance of the model in predicting new spikes and evaluation of the model-generated data compared to the actual data.

However, if the complexity of the model is still too high, we can reduce the overall number of parameters by eliminating the dependence of the model on time in trial and examine only the average effect over the entire trial. We can also simplify the model by eliminating the interactions between Rule and Attention Factors.

Another potential issue is the total number of simultaneously recorded neurons in each session. If there are relatively few neurons recorded simultaneously, they might form a poor approximation to the overall population firing rate. One possibility here is to use the LFPs instead, as in @kelly_accounting_2010.
